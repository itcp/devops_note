本次 安装 使用 5 台 Linux 服务器， 系统 版本 为 CentOS 7. 5， 分为 3 台 Master、 2 台 Node， 其中 Node 的 配置 相同。 Master 节点 主要 部署 的 组件 有 KeepAlived、 HAProxy、 Etcd、 Kubelet、 APIServer、 Controller、 Scheduler， Node 节点 主要 部署 的 为 Kubelet， 详情 见表 1- 1。 其中 的 概念 可以 参考 第 2 章 Docker 和 Kubernetes 基础 部分 的 内容。

表 1- 1 　 高 可用 Kubernetes 集群 规划 各 节点 通信 采用 主 机名 的 方式， 这种 方式 与 IP



| 主机名          | IP地址 |       说明       | 配置 |
| --------------- | -----: | :--------------: | ---- |
| K8s-master01~03 |        |   master节点x3   | 2C4G |
| K8S-master-lb   |        | keepalived虚拟IP |      |
| K8S-node01~02   |        |   worker节点x2   | 2C4G |
| K8S-kubectl     |        |   kubectl节点    | 1C1G |



各 节点 通信 采用 主 机名 的 方式， 这种 方式 与 IP 地址 相比较 更具 有 扩展性。 以下 介绍 具体 的 安装 步骤。



 所有 节点 配置 hosts， 修改/ etc/ hosts 如下：



```
cat /etc/hosts


```



**在云上操作,先创建一台 k8s-master 实例,部署好后克隆成3个实例



所有 节点 关闭 防火墙、 selinux、 dnsmasq、 swap（ 如果 开启 防火墙 需要 开放 对应 的 端口， 配置 较为 复杂）。 如果 在 云 上 部署， 可以 通过 安全 组 进行 安全 配置。 

服务器 配置 如下： 

关闭 Selinux setenforce 0 

将/ etc/ sysconfig/ selinux 文件 中的 SELINUX 改为 disabled： 

```
[root@k8s-master-01 ~]# setenforce 0
[root@k8s-master-01 ~]# cat /etc/selinux/config

# This file controls the state of SELinux on the system.
# SELINUX= can take one of these three values:
#     enforcing - SELinux security policy is enforced.
#     permissive - SELinux prints warnings instead of enforcing.
#     disabled - No SELinux policy is loaded.
SELINUX=disabled
# SELINUXTYPE= can take one of three values:
#     targeted - Targeted processes are protected,
#     minimum - Modification of targeted policy. Only selected processes are protected.
#     mls - Multi Level Security protection.
SELINUXTYPE=targeted


[root@k8s-master-01 ~]# 
```



关闭 swap 分区 swapoff -a && sysctl -w vm. swappiness=

```
swapoff -a && sysctl -w vm. swappiness= 0

```



注释/etc/fstab中的 swap 挂 载 选项：



所有 节点 同步 时间。 所有 节点 同步 时间 是 必须的， 并且 需要 加到 开机 自 启动 和 计划 任务 中， 如果 节点 时间 不 同步， 会 造成 Etcd 存储 Kubernetes 信息 的 键－值（ key- value） 数据库 同步 数据 不正常， 也会 造成 证书 出现 问题。 时间 同步 配置 如下：



```

```



所有 节点 配置 limit：

` ulimit -SHn 65535`



Master01 节点 免 密钥 登录 其他 节点， 安装 过程中 生成 配置文件 和 证书 均 在 Master01 上 操作， 集群管理 也在 Master01 上 操作， 阿 里 云 或者 AWS 上 需要 单独 一台 kubectl 服务器。 密钥 配置 如下：

```
[root@K8S-kubectl ~]#
ssh-keygen -t rsa
for i in (K8S-master-01 K8S-master-02 K8S-master-03 K8S-node-01 K8S-node-02 );do ssh-copy-id -i .ssh/id_rsa.pub $i; done
```



所有 节点 配置 repo 源：

** 由于我是境外主机,所以不用删除centos官方的源

```
yum install -y epel-release
```







所有 节点 升级 系统 并重 启， 此处 升级 没有 升级 内核， 下 节会 单独 升级 内核：



```
[root@k8s-master-01 ~]# cat /etc/centos-release
CentOS Linux release 7.6.1810 (Core)
[root@k8s-master-01 ~]# uname -r
3.10.0-957.1.3.el7.x86_64
[root@k8s-master-01 ~]#

yum install wget jq psmisc vim net-tools -y 

yum update -y --exclude=kernel* && reboot
```







## 内核升级

在 安装 过程中， 很多 文档 及 网上 资源 不会 提及 到 内核 升级 的 部分， 但 升级 内核 可以 减少 一些 不必 要的 Bug， 也是 安装 过程中 颇为 重要的 一步。 本例 升级 的 内核 版本 为 4. 18， 采用 rpm 的 安装 方式， Master01 节点 下载 内核 升级 包：

```

wget http://mirror.rc.usf.edu/compute_lock/elrepo/kernel/el7/x86_64/RPMS/kernel-ml-4.18.9-1.el7.elrepo.x86_64.rpm

wget http://mirror.rc.usf.edu/compute_lock/elrepo/kernel/el7/x86_64/RPMS/kernel-ml-4.18.9-1.el7.elrepo.x86_64.rpm
```

下面是4.19的,书中是用 4.18

```
wget http://mirror.rc.usf.edu/compute_lock/elrepo/kernel/el7/x86_64/RPMS/kernel-ml-4.19.0-1.el7.elrepo.x86_64.rpm  # 如果这个是报错没有文件,那就内核版本又要写大点,说明这个不是新的
wget http://mirror.rc.usf.edu/compute_lock/elrepo/kernel/el7/x86_64/RPMS/kernel-ml-4.19.0-1.el7.elrepo.x86_64.rpm

```



```
[root@k8s-master-01 ~]# ls -l
total 59064
-rw-------. 1 root root     6921 Jan 28  2019 anaconda-ks.cfg
-rw-r--r--  1 root root 47840760 Oct 22  2018 kernel-ml-4.19.0-1.el7.elrepo.x86_64.rpm
-rw-r--r--  1 root root 12622616 Oct 22  2018 kernel-ml-devel-4.19.0-1.el7.elrepo.x86_64.rpm
[root@k8s-master-01 ~]# yum localinstall -y kernel-ml*
```



修改 内核 启动 顺序：

```
grub2-set-default 0 && grub2-mkconfig -o /etc/grub2.cfg
grubby --args="user_namespace.enable=1" --update=kernel="$(grubby --default-kernel)"
```

```
[root@ip-172-31-4-42 ~]# grub2-set-default 0 && grub2-mkconfig -o /etc/grub2.cfg
Generating grub configuration file ...
Found linux image: /boot/vmlinuz-4.18.9-1.el7.elrepo.x86_64
Found initrd image: /boot/initramfs-4.18.9-1.el7.elrepo.x86_64.img
Found linux image: /boot/vmlinuz-3.10.0-957.1.3.el7.x86_64
Found initrd image: /boot/initramfs-3.10.0-957.1.3.el7.x86_64.img
Found linux image: /boot/vmlinuz-0-rescue-05cb8c7b39fe0f70e3ce97e5beab809d
Found initrd image: /boot/initramfs-0-rescue-05cb8c7b39fe0f70e3ce97e5beab809d.img
done
[root@ip-172-31-4-42 ~]# grubby --args="user_namespace.enable=1" --update=kernel="$(grubby --default-kernel)"
grubby: bad argument --update=kernel=/boot/vmlinuz-4.18.9-1.el7.elrepo.x86_64: unknown option
[root@ip-172-31-4-42 ~]#
```



重启主机

`reboot`



启动后确认现内核

`uname -r`

```
[centos@k8s-master-01 ~]$ uname -r
4.18.9-1.el7.elrepo.x86_64
[centos@k8s-master-01 ~]$
```



本书 的 Kube- Proxy 均 采用 ipvs 模式， 该 模式 也是 新版 默认 支持 的 代理 模式， 性能 比 iptables 要 高， 如果 服务器 未 配置 安装 ipvs， 将 转换 为 iptables 模式。 所有 节点 安装 ipvsadm：

` yum install ipvsadm ipset sysstat conntrack libseccomp -y`



所有 节点 配置 ipvs 模块， 在 内核 4. 19 版本 nf_ conntrack_ ipv4 已经 改为 nf_ conntrack:



检查 是否 加载， 并将 其 加入 至 开机 自动 加载（ 在 目录/ etc/ sysconfig/ modules/ 下 创建 一个 k8s. modules 写上 上述 命令 即可）：



```
lsmod | grep -e ip_vs -e nf_conntrack
```

```
[root@ip-172-31-4-42 ~]# ipvsadm
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
  -> RemoteAddress:Port           Forward Weight ActiveConn InActConn
[root@ip-172-31-4-42 ~]#
[root@ip-172-31-4-42 ~]# lsmod |grep -e ip_vs -e nf_conntrack
ip_vs                 151552  0
nf_conntrack          135168  1 ip_vs
libcrc32c              16384  4 nf_conntrack,btrfs,xfs,ip_vs
[root@ip-172-31-4-42 ~]#
```



开启 一些 K8S 集群 中 必须的 内核 参数， 所有 节点 配置 K8S 内核：

```

```







